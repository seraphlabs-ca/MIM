


<!DOCTYPE html><html lang="en" xml:lang="en">
<head><meta http-equiv="content-type" content="text/html; charset=utf-8" />
<meta charset="utf-8" >
<meta name="viewport" content="width=device-width, initial-scale=1.0"/>
<meta name="robots" content="index, follow"/>
<meta name="description" content="Seraph Labs Generated Content"/>
<title>Seraph Labs</title>

<link rel="apple-touch-icon" sizes="57x57" href="https://cdn.seraphlabs.ca/media/seraphlabs.ico/apple-icon-57x57.png"/>
<link rel="apple-touch-icon" sizes="60x60" href="https://cdn.seraphlabs.ca/media/seraphlabs.ico/apple-icon-60x60.png"/>
<link rel="apple-touch-icon" sizes="72x72" href="https://cdn.seraphlabs.ca/media/seraphlabs.ico/apple-icon-72x72.png"/>
<link rel="apple-touch-icon" sizes="76x76" href="https://cdn.seraphlabs.ca/media/seraphlabs.ico/apple-icon-76x76.png"/>
<link rel="apple-touch-icon" sizes="114x114" href="https://cdn.seraphlabs.ca/media/seraphlabs.ico/apple-icon-114x114.png"/>
<link rel="apple-touch-icon" sizes="120x120" href="https://cdn.seraphlabs.ca/media/seraphlabs.ico/apple-icon-120x120.png"/>
<link rel="apple-touch-icon" sizes="144x144" href="https://cdn.seraphlabs.ca/media/seraphlabs.ico/apple-icon-144x144.png"/>
<link rel="apple-touch-icon" sizes="152x152" href="https://cdn.seraphlabs.ca/media/seraphlabs.ico/apple-icon-152x152.png"/>
<link rel="apple-touch-icon" sizes="180x180" href="https://cdn.seraphlabs.ca/media/seraphlabs.ico/apple-icon-180x180.png"/>
<link rel="icon" type="image/png" sizes="192x192"  href="https://cdn.seraphlabs.ca/media/seraphlabs.ico/android-icon-192x192.png"/>
<link rel="icon" type="image/png" sizes="32x32" href="https://cdn.seraphlabs.ca/media/seraphlabs.ico/favicon-32x32.png"/>
<link rel="icon" type="image/png" sizes="96x96" href="https://cdn.seraphlabs.ca/media/seraphlabs.ico/favicon-96x96.png"/>
<link rel="icon" type="image/png" sizes="16x16" href="https://cdn.seraphlabs.ca/media/seraphlabs.ico/favicon-16x16.png"/>
<link rel="manifest" href="https://cdn.seraphlabs.ca/media/seraphlabs.ico/manifest.json" crossorigin="anonymous"/>
<meta name="msapplication-TileColor" content="#ffffff"/>
<meta name="msapplication-TileImage" content="https://cdn.seraphlabs.ca/media/seraphlabs.ico/ms-icon-144x144.png"/>
<meta name="theme-color" content="#ffffff"/><script type="text/javascript">function isTouchDevice(){return'ontouchstart'in document.documentElement;}</script><script type="text/x-mathjax-config">MathJax.Hub.Config({"HTML-CSS":{preferredFont:"TeX",availableFonts:["STIX","TeX"],linebreaks:{automatic:true},EqnChunk:(MathJax.Hub.Browser.isMobile?10:50)},tex2jax:{inlineMath:[["\\$","\\$"],["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:true,ignoreClass:"tex2jax_ignore|dno"},TeX:{noUndefined:{attributes:{mathcolor:"red",mathbackground:"#FFEEEE",mathsize:"90%"}},equationNumbers:{autoNumber:"all"}},messageStyle:"none"});</script><script type="text/javascript"async src="https://cdn.seraphlabs.ca/js/MathJax//MathJax.js?config=TeX-AMS_HTML"></script><script type="text/javascript"src="https://cdn.seraphlabs.ca/js/jquery.js/jquery.min.js"></script><link rel="stylesheet"href="https://cdn.seraphlabs.ca/js/jquery-ui.js/jquery-ui.min.css"/><script type="text/javascript"src="https://cdn.seraphlabs.ca/js/jquery-ui.js/jquery-ui.min.js"></script><script type="text/javascript">if(isTouchDevice()){$.getScript("https://cdn.seraphlabs.ca/js/jquery-ui-touch-punch/jquery.ui.touch-punch.min.js");}</script><script type="text/javascript"src="https://cdn.seraphlabs.ca/js/toc.js/dist/toc.min.js"></script><script type="text/javascript"src="https://cdn.seraphlabs.ca/js/jquery-searchable/dist/jquery.searchable-1.1.0.min.js"></script><script type="text/javascript"charset="utf-8"src="https://cdn.seraphlabs.ca/js/dataTables.js/media/js/jquery.dataTables.min.js"></script><link rel="stylesheet"type="text/css"href="https://cdn.seraphlabs.ca/js/dataTables.js/media/css/jquery.dataTables.min.css"/><link rel="stylesheet"href="https://cdn.seraphlabs.ca/js/chosen.js/docsupport/prism.css"/><link rel="stylesheet"href="https://cdn.seraphlabs.ca/js/chosen.js/chosen.css"/><link rel="stylesheet"href="https://cdn.seraphlabs.ca/js/code-prettify.js/loader/prettify.css"/><script type="text/javascript"src="https://cdn.seraphlabs.ca/js/code-prettify.js/loader/run_prettify.js"></script><script src="https://cdn.seraphlabs.ca/js/chosen.js/chosen.jquery.min.js"type="text/javascript"></script><script src="https://cdn.seraphlabs.ca/js/chosen.js/docsupport/prism.js"type="text/javascript"charset="utf-8"></script>
<style type="text/css">body{font-family:Helvetica,Arial;width:100vw;height:100vh;z-index:0}a img{text-decoration:none}.hidden{visibility:hidden}.errors-form-field{color:red}.text-center{text-align:center}.hcontainer{display:table;text-align:center}.vcontainer{display:table-cell;vertical-align:middle}.center{margin:auto;text-align:center}.fit{max-width:100vw;max-height:100vh;width:100%;height:100%;object-fit:contain;display:flex}.box{-webkit-border-radius:4px;-moz-border-radius:4px;border-radius:4px}.mpld3-figure{width:auto;height:auto;transform-origin:0 0}video{width:80%;height:100vh}.image-container{position:relative;display:flex;width:100vw;height:100vh}.image-row-1{width:100%;float:left}.image-row-2{width:50%;float:left}.image-row-3{width:33%;float:left}.image-row-4{width:25%;float:left}.image-row-5{width:25%;float:left}.image-row-6{width:16.66%;float:left}.text{margin:10px}.round-border{background-color:rgba(185,185,185,.5);-webkit-border-radius:20px;-moz-border-radius:20px;border-radius:20px;padding:10px}#main{top:0;left:0;z-index:0;padding-left:1%;overflow-x:auto}#main .prettyprint{width:99%;overflow-x:scroll}#main.main-without-toc{width:98%}#main.main-with-toc{width:78%}#main.main-with-toc.touch-screen{width:98%}#main.touch-screen{-webkit-overflow-scrolling:touch;overflow-x:visible}#main h1{background-color:#fcbb12;-webkit-border-radius:20px;-moz-border-radius:20px;border-radius:20px;padding:10px;clear:both}#main h2{background-color:#008bcd;-webkit-border-radius:20px;-moz-border-radius:20px;border-radius:20px;padding:10px;clear:both}#main h3{background-color:#009f50;-webkit-border-radius:20px;-moz-border-radius:20px;border-radius:20px;padding:10px;clear:both}#main-header{width:100%;margin:auto}div#main-header a:-webkit-any-link{text-decoration:none}div#main-header.touch-screen .logo-image{width:100%}#main-content{width:100%;padding-bottom:10px}body{counter-reset:hcounter}#main-content h1:before{content:counter(hcounter);counter-increment:hcounter;color:rgba(100,100,100,1.0);margin-right:20px;padding:2px;padding-left:10px;padding-right:10px;clear:both}#main-content h1.nocount:before{content:none;counter-increment:none}#main-content h2:before{content:counter(hcounter);counter-increment:hcounter;color:rgba(100,100,100,1.0);margin-right:20px;padding:2px;padding-left:10px;padding-right:10px;clear:both}#main-content h2.nocount:before{content:none;counter-increment:none}#main-content h3:before{content:counter(hcounter);counter-increment:hcounter;color:rgba(100,100,100,1.0);margin-right:20px;padding:2px;padding-left:10px;padding-right:10px;clear:both}#main-content h3.nocount:before{content:none;counter-increment:none}#main-footer{text-align:center;display:block}div#main-footer p{display:inline;font-size:x-small;font-weight:bold}#toc{height:99vh;margin-right:1%;position:fixed;top:0;right:0;overflow-y:scroll;overflow-x:auto;z-index:1;background:#fff8e1;border-radius:8px;border:solid 2px #ffc300}#toc li.toc-active{margin-right:4px;border-radius:8px;background-color:rgba(132,133,141,0.25)}#toc.toc-active{opacity:1.0;width:18%}#toc.toc-inactive{opacity:.0;width:0}#toc-toggle-button{font-weight:bolder}.toc-button-active::before{content:"⇨"}.toc-button-inactive::before{content:"⇦"}#toc-toggle-button{z-index:4;position:fixed;width:20px;height:20px;top:1%;right:1.5%;padding:2px;text-align:center;background:#00ad4d;-webkit-border-radius:10px;-moz-border-radius:10px;border-radius:10px;border:ridge 2px #20538d;opacity:.5}#toc.touch-screen{width:96%;-webkit-overflow-scrolling:touch}#toc li[class^="h"]{padding:5px 5px;word-wrap:break-word}#toc ul{margin:0;padding:10px 10px;list-style:none}#toc a{color:#333;text-decoration:none}#toc li:before{content:"► ";font-family:"Arial Black"}#toc li.h1:before{color:#fcbb12}#toc li.h2:before{color:#008bcd}#toc li.h3:before{color:#009f50}#toc li.h1{margin-left:0}#toc li.h2{margin-left:10px}#toc li.h3{margin-left:20px}#toc li.h4{margin-left:30px}#toc li.h5{margin-left:40px}#toc li.h6{margin-left:50px}#toc-header{width:100%;padding-top:10px;text-align:center;font-size:small;color:#756503}#toc-header-help-message{opacity:.5}#toc-content{width:100%;margin:auto}#toc-content-search-container{width:100%;margin-top:5px;margin-bottom:-5px}.toc-hidden-search-element{visibility:collapse;height:0 !important;width:0 !important;padding:0 !important;margin:0 !important}.search-toc-active{opacity:1.0}.search-toc-inactive{opacity:.3}#sidebar{z-index:3;position:fixed;top:0;left:0;bottom:0;width:30%;overflow-y:scroll;overflow-x:auto;padding:1%;background:#bfbfbf;-webkit-border-radius:4px;-moz-border-radius:4px;border-radius:4px;border:ridge 2px #20538d;-webkit-box-shadow:inset 0 1px 0 rgba(255,255,255,0.4),0 1px 1px rgba(0,0,0,0.2);-moz-box-shadow:inset 0 1px 0 rgba(255,255,255,0.4),0 1px 1px rgba(0,0,0,0.2);box-shadow:inset 0 1px 0 rgba(255,255,255,0.4),0 1px 1px rgba(0,0,0,0.2);opacity:.8;visibility:hidden}#sidebar-draggable{z-index:4;position:fixed;display:inline-block;text-align:center;width:fit-content;top:1%;left:1%;padding:5px;user-select:none;font-weight:bolder;font-size:xx-small;background:#00ad4d;-webkit-border-radius:10px;-moz-border-radius:10px;border-radius:10px;border:ridge 2px #20538d;opacity:.5}.fancy-header{color:white;text-shadow:-2px 0 black,0 2px black,2px 0 black,0 -2px black,-2px -2px black,-2px 2px black,2px -2px black,2px 2px black,0 0 5px rgba(0,0,0,.05),1px 1px 3px rgba(0,0,0,.2),3px 3px 5px rgba(0,0,0,.2),5px 5px 10px rgba(0,0,0,.2),10px 10px 10px rgba(0,0,0,.2),20px 20px 20px rgba(0,0,0,.3)}#main-header-title{text-align:center;margin:auto;overflow:none}#main-header-title a{font-weight:800;font-variant:small-caps;font-size:50px;padding-top:10px;padding-bottom:10px}#main-header-body{text-align:center;display:inline-block;padding-top:10px;padding-bottom:10px;width:100%;overflow:auto}#main-header-logo{text-align:center;text-decoration:none;margin:auto;width:100%;padding-top:10px;padding-bottom:10px;overflow:auto}div#main-header-logo a{text-decoration:none}#toc-header-title{text-align:center;margin:auto}#main-content{font-size:x-large}pre{background:lightgray;-webkit-border-radius:20px;-moz-border-radius:20px;border-radius:20px;padding:10px}</style>
</head>


<body>

  <div id="toc-toggle-button" class="toc-button-inactive">
  </div>

  <div id="toc" class='toc-inactive hidden'>
    <div id="toc-header">
      <div id="toc-header-help-message">
      <b>Toggle TOC (ESC)</b>
      </div>
<div id="toc-header-title">
</div>
    </div>
    <div id="toc-content-search-container" class="hcontainer">
        <input type="search" id="toc-content-search" value="" class="search-toc-inactive" placeholder="Search TOC...">
    </div>
    <div id="toc-content" class="toc">
    </div>
  </div>


  <div id="main" class="main-without-toc">
    <div id="main-header">
<div id="main-header-logo">
</div>

<div id="main-header-title">
<a href="?" class="fancy-header">MIM: Mutual Information Machine
</a>
</div>

<div id="main-header-body">
<p><b>Date: </b>09/2019</p>
<p><b>Author(s): </b><br>
<div style="width: 80%; text-align: center; display: inline-block;">

<div style="width: 20%; min-width: 230px; text-align: center; display: inline-block;vertical-align:middle; margin: 10px;">
Micha Livne
<br>
<a href="mailto:livne@seraphlabs.ca">livne@seraphlabs.ca</a>
<br>
University of Toronto
<br>
Vector Institute
</div>

<div style="width: 20%; min-width: 230px; text-align: center; display: inline-block;vertical-align:middle; margin: 10px;">
Kevin Swersky
<br>
<a href="mailto:lkswersky@google.com">lkswersky@google.com</a>
<br>
Google Research
</div>

<div style="width: 20%; min-width: 230px; text-align: center; display: inline-block;vertical-align:middle; margin: 10px;">
David J. Fleet
<br>
<a href="mailto:fleet@cs.toronto.edu">fleet@cs.toronto.edu</a>
<br>
University of Toronto
<br>
Vector Institute
</div>

</div>
</p>
<p><b>Last Updated: </b>16/10/2020 16:33:54</p>
</div>

    </div>
    <div id="main-content">
<h1>MIM: Mutual Information Machine</h1>

<ul>
<li>Viewing this README.md in the <a href="https://github.com/seraphlabs-ca/MIM" target="_blank">github repo</a> will suffer from formatting issues. We recommend instead to view <a href="https://research.seraphlabs.ca/projects/mim/index.html" target="_blank">index.html</a> (can also be viewed locally).</li>
</ul>

<h2>Links</h2>

<ul>
<li><a href="https://github.com/seraphlabs-ca/MIM" target="_blank">github repo</a></li>
<li><a href="https://research.seraphlabs.ca/projects/mim/index.html" target="_blank">Project webpage</a></li>
<li><a href="https://arxiv.org/abs/1910.03175" target="_blank">Preprint paper</a></li>
<li><a href="https://research.seraphlabs.ca/presentations/mim-paper" target="_blank">Presentation</a></li>
</ul>

<h2>Why should you care? Posterior Collapse!</h2>

<div style="text-align: center; display:inline-block;">
    <div style="width: 30%; display:inline-block;">
        <p style="text-align: center;">AE (High MI, No Latent Prior)</p>
        <img width="100%" alt="AE" src="images/show-off/toyAE_z2_ae_logvar6_mid-dim50_layers2_q-x0marginal_q-zx0_p-z0anchor_p-xz0_progress_reconstruction-video.gif">
    </div>
    <div style="width: 30%; display:inline-block;">
        <p style="text-align: center;">MIM (High MI, Latent Prior Alignment)</p>
        <img width="100%" alt="MIM" src="images/show-off/toyMIM_z2_mim-samp_logvar6_mid-dim50_layers2_q-x0marginal_q-zx0_p-z0anchor_p-xz0_progress_reconstruction-video.gif">
    </div>
    <div style="width: 30%; display:inline-block;">
        <p style="text-align: center;">VAE (High MI, Latent Prior Regularization)</p>
        <img width="100%"  alt="VAE" src="images/show-off/toyVAE_z2_vae_logvar6_mid-dim50_layers2_q-x0marginal_q-zx0_p-z0anchor_p-xz0_progress_reconstruction-video.gif">
    </div>

    <p style="text-align: left; width: 60%; margin: auto;">
    MIM and VAE models with 2D inputs, and 2D latent space.
    <br>
    Top row: <b>Black</b> contours depict level sets of P(x); <span style="color: red">red</span> dots are reconstructed test points.
    <br>
    Bottom row: <span style="color: green">Green</span> contours are one standard deviation ellipses of q(z|x) for test points. Dashed black circles depict one standard deviation of P(z).
    <br>
    <br>
    </p>
    <ul style="text-align: left; width: 60%; margin: auto;">
        <li>AE (auto-encoder) produces zero predictive variance (i.e., delta function) and lower reconstruction errors, consistent with high mutual information. The structure in the latent space is the result of the architecture inductive bias. The lack of a prior leads to an undetermined alignment with P(z) (i.e., an arbitrary structure in the latent space).</li>
        <li>MIM produces lower predictive variance and lower reconstruction errors, consistent with high mutual information, alongside alignment with P(z) (i.e., structured latent space).</li>
        <li>VAE is optimized with annealing of beta in beta-VAE. Once annealing is completed (i.e., beta = 1), the VAE posteriors show  high predictive variance, which is indicative of partial posterior collapse. The increased variance leads to reduced mutual information and worse reconstruction error as a result of a strong alignment with P(Z) (i.e, overly structured/regularized latent space).</li>
    </ul>
</div>

<h2>Requirements</h2>

<p>The code has been tested on CPU and NVIDIA Titan Xp GPU, using Anaconda, Python 3.6, and zsh:</p>

<pre><code># tools
zsh 5.4.2
Cuda compilation tools, release 9.0, V9.0.176
conda 4.6.14
Python 3.6.8

# python packages (see requirements.txt for complete list)
scipy==1.1.0
matplotlib==3.0.3
numpy==1.15.4
torchvision==0.2.1
torch==1.0.0
scikit_learn==0.21.3
</code></pre>

<h2>Installation</h2>

<p>Please follow installation instructions in the following link: <a href="https://pytorch.org">pytorch</a>.</p>

<pre><code>pip install -r requirements.txt
</code></pre>

<h2>Data</h2>

<p>The experiments can be run on the following datasets:</p>

<ul>
<li><a href="http://yann.lecun.com/exdb/mnist/">binary MNIST</a></li>
<li><a href="https://github.com/yburda/iwae/blob/master/datasets/OMNIGLOT/chardata.mat">OMNIGLOT</a></li>
<li><a href="https://github.com/zalandoresearch/fashion-mnist">Fashion-MNIST</a></li>
</ul>

<p>All datasets are included as part of the repo for convenience.
Links are provided as a workaround (i.e., in case of issues).</p>

<h2>Experiments</h2>

<p>Directory structure (if code fail due to a missing directory please create manually):</p>

<pre><code>src/ - Experiments are assumed to be executed from this directory.
data/assets - Datasets will be saved here.
data/torch-generated - Results will be saved here.
</code></pre>

<p><strong>NOTE (if code fails due to CUDA/GPU issues):</strong> To prevent the use of CUDA/GPU and enforce CPU computation, please add the following flag to the supplied command lines below:</p>

<pre><code>--no-cuda
</code></pre>

<p>Otherwise, by default CUDA will be used, if detected by pytorch. </p>

<p>For detailed explanation of the plots below, please see the paper.</p>

<h3>Animation</h3>

<p>To produce the animation at the top:</p>

<pre><code># MIM
./vae-as-mim-dataset.py \
    --dataset toyMIM \
    --z-dim 2 \
    --mid-dim 50 \
    --min-logvar 6 \
    --seed 1 \
    --batch-size 128 \
    --epochs 49 \
    --warmup-steps 25 \
    --vis-progress \
    --mim-loss \
    --mim-samp
#VAE
./vae-as-mim-dataset.py \
    --dataset toyVAE \
    --z-dim 2 \
    --mid-dim 50 \
    --min-logvar 6 \
    --seed 1 \
    --batch-size 128 \
    --epochs 49 \
    --warmup-steps 25  \
    --vis-progress
#AE
./vae-as-mim-dataset.py \
    --dataset toyAE \
    --z-dim 2 \
    --mid-dim 50 \
    --min-logvar 6 \
    --seed 1 \
    --batch-size 128 \
    --epochs 49 \
    --warmup-steps 25  \
    --vis-progress \
    --ae-loss
</code></pre>

<h3>2D Experiments</h3>

<p>Experimenting with expressiveness of MIM and VAE:</p>

<pre><code>for seed in 1 2 3 4 5 6 7 8 9 10; do
        for mid_dim in 5 20 50 100 200 300 400 500; do
            # MIM
            ./vae-as-mim-dataset.py \
                --dataset toy4 \
                --z-dim 2 \
                --mid-dim ${mid_dim} \
                --min-logvar 6 \
                --seed ${seed} \
                --batch-size 128 \
                --epochs 200 \
                --warmup-steps 3 \
                --mim-loss \
                --mim-samp
            # VAE
            ./vae-as-mim-dataset.py \
                --dataset toy4 \
                --z-dim 2 \
                --mid-dim ${mid_dim} \
                --min-logvar 6 \
                --seed ${seed} \
                --batch-size 128 \
                --epochs 200 \
                --warmup-steps 3
        done
done
</code></pre>

<p>Results below demonstrate posterior collapse in VAE, and the lack of it in MIM.</p>

<div style="text-align: center; width: 100%;">
    <div style="width: 48%; display:inline-block; vertical-align:middle;">
        <p style="text-align: center;">MIM (5, 20, 500 hidden units)</p>
        <img alt="MIM" width="32% "src="images/toy4/plots/mim-samp_logvar6_mid-dim5_layers2_q-x0marginal_q-zx0_p-z0anchor_p-xz0/reconstruction_best.png">
        <img alt="MIM" width="32% "src="images/toy4/plots/mim-samp_logvar6_mid-dim20_layers2_q-x0marginal_q-zx0_p-z0anchor_p-xz0/reconstruction_best.png">
        <img alt="MIM" width="32% "src="images/toy4/plots/mim-samp_logvar6_mid-dim500_layers2_q-x0marginal_q-zx0_p-z0anchor_p-xz0/reconstruction_best.png">
    </div>
    <div style="width: auto; height: 200px; display:inline-block; vertical-align:middle; padding-left: 2%;"></div>
    <div style="width: 48%; display:inline-block; vertical-align:middle;">
        <p style="text-align: center;">VAE (5, 20, 500 hidden units)</p>
        <img alt="VAE" width="32% "src="images/toy4/plots/vae_logvar6_mid-dim5_layers2_q-x0marginal_q-zx0_p-z0anchor_p-xz0/reconstruction_best.png">
        <img alt="VAE" width="32% "src="images/toy4/plots/vae_logvar6_mid-dim20_layers2_q-x0marginal_q-zx0_p-z0anchor_p-xz0/reconstruction_best.png">
        <img alt="VAE" width="32% "src="images/toy4/plots/vae_logvar6_mid-dim500_layers2_q-x0marginal_q-zx0_p-z0anchor_p-xz0/reconstruction_best.png">
    </div>

    <div style="text-align: left; width: 9%; display: inline-block"><span style="color: blue;">MIM</span><br><span style="color: red;">VAE</span></div>
    <div style="width: 20%; display:inline-block;">
        <img width="100%" alt="MI" src="images/toy4/stats/fig.MI_ksg.png">
        <p style="text-align: center;">MI</p>    
    </div>
    <div style="width: 20%; display:inline-block;">
        <img width="100%" alt="NLL" src="images/toy4/stats/fig.H_q_x.png">
        <p style="text-align: center;">NLL</p>    
    </div>
    <div style="width: 20%; display:inline-block;">
        <img width="100%" alt="RMSE" src="images/toy4/stats/fig.x_recon_err.png">
        <p style="text-align: center;">Recon. RMSE</p>    
    </div>
    <div style="width: 20%; display:inline-block;">
        <img width="100%" alt="Cls. Acc." src="images/toy4/stats/fig.clf_acc_KNN5.png">
        <p style="text-align: center;">Classification Acc.</p>    
    </div>
</div>

<p>Experimenting with effect of entropy prior on MIM and VAE:</p>

<pre><code>for seed in 1 2 3 4 5 6 7 8 9 10; do
        for mid_dim in 5 20 50 100 200 300 400 500; do
                # MIM
                ./vae-as-mim-dataset.py \
                    --dataset toy4 \
                    --z-dim 2 \
                    --mid-dim ${mid_dim} \
                    --min-logvar 6 \
                    --seed ${seed} \
                    --batch-size 128 \
                    --epochs 200 \
                    --warmup-steps 3 \
                    --mim-loss \
                    --mim-samp \
                    --inv-H-loss
                # VAE
                ./vae-as-mim-dataset.py \
                    --dataset toy4 \
                    --z-dim 2 \
                    --mid-dim ${mid_dim} \
                    --min-logvar 6 \
                    --seed ${seed} \
                    --batch-size 128 \
                    --epochs 200 \
                    --warmup-steps 3 \
                    --inv-H-loss
        done
done
</code></pre>

<p>Results below demonstrate how adding joint entropy as regularizer can prevent posterior collapse in VAE, and subtracting the joint entropy can generate a strong collapse in MIM.</p>

<div style="text-align: center; width: 100%;">
    <div style="width: 48%; display:inline-block;">
        <p style="text-align: center;">MIM - H (5, 20, 500 hidden units)</p>
        <img alt="MIM" width="32% "src="images/toy4/plots/mim-samp_logvar6_mid-dim5_layers2_q-x0marginal_q-zx0_p-z0anchor_p-xz0-inv_H/reconstruction_best.png">
        <img alt="MIM" width="32% "src="images/toy4/plots/mim-samp_logvar6_mid-dim20_layers2_q-x0marginal_q-zx0_p-z0anchor_p-xz0-inv_H/reconstruction_best.png">
        <img alt="MIM" width="32% "src="images/toy4/plots/mim-samp_logvar6_mid-dim500_layers2_q-x0marginal_q-zx0_p-z0anchor_p-xz0-inv_H/reconstruction_best.png">
    </div>
    <div style="width: 48%; display:inline-block;">
        <p style="text-align: center;">VAE + H (5, 20, 500 hidden units)</p>
        <img alt="VAE" width="32% "src="images/toy4/plots/vae_logvar6_mid-dim5_layers2_q-x0marginal_q-zx0_p-z0anchor_p-xz0-inv_H/reconstruction_best.png">
        <img alt="VAE" width="32% "src="images/toy4/plots/vae_logvar6_mid-dim20_layers2_q-x0marginal_q-zx0_p-z0anchor_p-xz0-inv_H/reconstruction_best.png">
        <img alt="VAE" width="32% "src="images/toy4/plots/vae_logvar6_mid-dim500_layers2_q-x0marginal_q-zx0_p-z0anchor_p-xz0-inv_H/reconstruction_best.png">
    </div>

    <div style="text-align: left; width: 9%; display: inline-block"><span style="color: blue;">MIM</span><br><span style="color: red;">VAE</span></div>
    <div style="width: 20%; display:inline-block;">
        <img width="100%" alt="MI" src="images/toy4/stats-inv_H/fig.MI_ksg.png">
        <p style="text-align: center;">MI</p>    
    </div>
    <div style="width: 20%; display:inline-block;">
        <img width="100%" alt="NLL" src="images/toy4/stats-inv_H/fig.H_q_x.png">
        <p style="text-align: center;">NLL</p>    
    </div>
    <div style="width: 20%; display:inline-block;">
        <img width="100%" alt="RMSE" src="images/toy4/stats-inv_H/fig.x_recon_err.png">
        <p style="text-align: center;">Recon. RMSE</p>    
    </div>
    <div style="width: 20%; display:inline-block;">
        <img width="100%" alt="Cls. Acc." src="images/toy4/stats-inv_H/fig.clf_acc_KNN5.png">
        <p style="text-align: center;">Classification Acc.</p>    
    </div>
</div>

<h3>Bottleneck</h3>

<p>Experimenting with effect of bottleneck on VAE and MIM.</p>

<h4>20D with 5 GMM</h4>

<p>A synthetic 5 GMM dataset with 20D x:</p>

<pre><code>for seed in 1 2 3 4 5 6 7 8 9 10; do
        for z_dim in 2 4 6 8 10 12 14 16 18 20; do
            # MIM
            ./vae-as-mim-dataset.py \
                --dataset toy4_20  \
                --z-dim ${z_dim}  \
                --mid-dim 50  \
                --seed ${seed}  \
                --epochs 200   \
                --min-logvar 6  \
                --warmup-steps 3   \
                --mim-loss  \
                --mim-samp
            # VAE
            ./vae-as-mim-dataset.py  \
                --dataset toy4_20   \
                --z-dim ${z_dim}  \
                --mid-dim 50  \
                --seed ${seed}  \
                --epochs 200  \
                --min-logvar 6  \
                --warmup-steps 3
        done
done
</code></pre>

<p>Results below demonstrate posterior collapse in VAE which worsen as the latent dimensionality increases, and the lack of it in MIM.</p>

<div style="text-align: center; width: 100%;">
    <div style="text-align: left; width: 9%; display: inline-block"><span style="color: blue;">MIM</span><br><span style="color: red;">VAE</span></div>
    <div style="width: 20%; display:inline-block;">
        <img width="100%" alt="MI" src="images/toy4_20/stats/fig.MI_ksg.png">
        <p style="text-align: center;">MI</p>    
    </div>
    <div style="width: 20%; display:inline-block;">
        <img width="100%" alt="NLL" src="images/toy4_20/stats/fig.H_q_x.png">
        <p style="text-align: center;">NLL</p>    
    </div>
    <div style="width: 20%; display:inline-block;">
        <img width="100%" alt="RMSE" src="images/toy4_20/stats/fig.x_recon_err.png">
        <p style="text-align: center;">Recon. RMSE</p>    
    </div>
    <div style="width: 20%; display:inline-block;">
        <img width="100%" alt="Cls. Acc." src="images/toy4_20/stats/fig.clf_acc_KNN5.png">
        <p style="text-align: center;">Classification Acc.</p>    
    </div>
</div>

<h4>20D with Fashion-MNIST PCA</h4>

<p>A PCA reduction of Fashion-MNIST to 20D x:</p>

<pre><code>for seed in 1 2 3 4 5 6 7 8 9 10; do
    for z_dim in 2 4 6 8 10 12 14 16 18 20; do
            # MIM
            ./vae-as-mim-dataset.py  \
                --dataset pca-fashion-mnist20   \
                --z-dim ${z_dim}  \
                --mid-dim 50  \
                --seed ${seed}  \
                --epochs 200  \
                --min-logvar 6  \
                --warmup-steps 3  \
                --mim-loss  \
                --mim-samp
            # VAE
            ./vae-as-mim-dataset.py  \
                --dataset pca-fashion-mnist20   \
                --z-dim ${z_dim}  \
                --mid-dim 50  \
                --seed ${seed}  \
                --epochs 200  \
                --min-logvar 6  \
                --warmup-steps 3
        done
done
</code></pre>

<p>Results below demonstrate posterior collapse in VAE which worsen as the latent dimensionality increases, and the lack of it in MIM. Here, for real-world data observations.</p>

<div style="text-align: center; width: 100%;">
    <div style="text-align: left; width: 9%; display: inline-block"><span style="color: blue;">MIM</span><br><span style="color: red;">VAE</span></div>
    <div style="width: 20%; display:inline-block;">
        <img width="100%" alt="MI" src="images/pca-fashion-mnist20/stats/fig.MI_ksg.png">
        <p style="text-align: center;">MI</p>    
    </div>
    <div style="width: 20%; display:inline-block;">
        <img width="100%" alt="NLL" src="images/pca-fashion-mnist20/stats/fig.H_q_x.png">
        <p style="text-align: center;">NLL</p>    
    </div>
    <div style="width: 20%; display:inline-block;">
        <img width="100%" alt="RMSE" src="images/pca-fashion-mnist20/stats/fig.x_recon_err.png">
        <p style="text-align: center;">Recon. RMSE</p>    
    </div>
    <div style="width: 20%; display:inline-block;">
        <img width="100%" alt="Cls. Acc." src="images/pca-fashion-mnist20/stats/fig.clf_acc_KNN5.png">
        <p style="text-align: center;">Classification Acc.</p>    
    </div>
</div>

<h3>High Dimensional Image Data</h3>

<p>Experimenting with high dimensional image data where we cannot reliably measure mutual information:</p>

<pre><code>for seed in 1 2 3 4 5 6 7 8 9 10; do
    for dataset_name in dynamic_mnist dynamic_fashion_mnist omniglot; do
        for model_name in convhvae_2level convhvae_2level-smim pixelhvae_2level pixelhvae_2level-amim; do
            for prior in vampprior standard; do
                ./vae-as-mim-image.py \
                    --dataset_name ${dataset_name} \
                    --model_name ${model_name} \
                    --prior ${prior} \
                    --seed ${seed} \
                    --use_training_data_init
            done
        done
    done
done
</code></pre>

<p>Results below demonstrate comparable sampling and reconstruction of VAE and MIM, and better unsupervised clustering for MIM, as a result of higher mutual information.</p>

<div style="text-align: center; width: 100%;">
    <div style="width: 8%; height: 100%; display:inline-block;"></div>
    <div style="width: 90%; display:inline-block;">
        <div style="width: 30%; display:inline-block; vertical-align:middle;">
            <p style="text-align: center;">Samples</p>    
        </div>
        <div style="width: 30%; display:inline-block; vertical-align:middle;">
            <p style="text-align: center;">Reconstruction</p>    
        </div>
        <div style="width: 30%; display:inline-block; vertical-align:middle;">
            <p style="text-align: center;">Latent Embeddings</p>    
        </div>
    </div>
    <div style="width: 8%; height: 100%; display:inline-block;">MIM</div>
    <div style="width: 90%; display:inline-block; vertical-align:middle;">
        <div style="width: 30%; display:inline-block; vertical-align:middle;">
            <img width="100%" alt="MIM Samples" src="images/dynamic_fashion_mnist_pixelhvae_2level-amim_vampprior__K_500__wu_100__z1_40_z2_40/generations.png">
        </div>
        <div style="width: 30%; display:inline-block; vertical-align:middle;">
            <img width="100%" alt="MIM Recon." src="images/dynamic_fashion_mnist_pixelhvae_2level-amim_vampprior__K_500__wu_100__z1_40_z2_40/real_recon.png">
        </div>
        <div style="width: 30%; display:inline-block; vertical-align:middle;">
            <img width="100%" alt="MIM Z Embed" src="images/dynamic_fashion_mnist_pixelhvae_2level-amim_vampprior__K_500__wu_100__z1_40_z2_40/z_embed.png">
        </div>
    </div>
    <div style="width: 8%; height: 100%; display:inline-block;">VAE</div>
    <div style="width: 90%; display:inline-block; vertical-align:middle;">
        <div style="width: 30%; display:inline-block; vertical-align:middle;">
            <img width="100%" alt="VAE Samples" src="images/dynamic_fashion_mnist_pixelhvae_2level_vampprior__K_500__wu_100__z1_40_z2_40/generations.png">
        </div>
        <div style="width: 30%; display:inline-block; vertical-align:middle;">
            <img width="100%" alt="VAE Recon." src="images/dynamic_fashion_mnist_pixelhvae_2level_vampprior__K_500__wu_100__z1_40_z2_40/real_recon.png">
        </div>
        <div style="width: 30%; display:inline-block; vertical-align:middle;">
            <img width="100%" alt="VAE Z Embed" src="images/dynamic_fashion_mnist_pixelhvae_2level_vampprior__K_500__wu_100__z1_40_z2_40/z_embed.png">
        </div>
    </div>
    <p style="text-align: center;"><b>Fashion-MNIST</b></p>    
</div>

<div style="text-align: center; width: 100%;">
    <div style="width: 8%; height: 100%; display:inline-block;"></div>
    <div style="width: 90%; display:inline-block;">
        <div style="width: 30%; display:inline-block; vertical-align:middle;">
            <p style="text-align: center;">Samples</p>    
        </div>
        <div style="width: 30%; display:inline-block; vertical-align:middle;">
            <p style="text-align: center;">Reconstruction</p>    
        </div>
        <div style="width: 30%; display:inline-block; vertical-align:middle;">
            <p style="text-align: center;">Latent Embeddings</p>    
        </div>
    </div>
    <div style="width: 8%; height: 100%; display:inline-block;">MIM</div>
    <div style="width: 90%; display:inline-block; vertical-align:middle;">
        <div style="width: 30%; display:inline-block; vertical-align:middle;">
            <img width="100%" alt="MIM Samples" src="images/dynamic_mnist_pixelhvae_2level-amim_vampprior__K_500__wu_100__z1_40_z2_40/generations.png">
        </div>
        <div style="width: 30%; display:inline-block; vertical-align:middle;">
            <img width="100%" alt="MIM Recon." src="images/dynamic_mnist_pixelhvae_2level-amim_vampprior__K_500__wu_100__z1_40_z2_40/real_recon.png">
        </div>
        <div style="width: 30%; display:inline-block; vertical-align:middle;">
            <img width="100%" alt="MIM Z Embed" src="images/dynamic_mnist_pixelhvae_2level-amim_vampprior__K_500__wu_100__z1_40_z2_40/z_embed.png">
        </div>
    </div>
    <div style="width: 8%; height: 100%; display:inline-block;">VAE</div>
    <div style="width: 90%; display:inline-block; vertical-align:middle;">
        <div style="width: 30%; display:inline-block; vertical-align:middle;">
            <img width="100%" width="100%" alt="VAE Samples" src="images/dynamic_mnist_pixelhvae_2level_vampprior__K_500__wu_100__z1_40_z2_40/generations.png">
        </div>
        <div style="width: 30%; display:inline-block; vertical-align:middle;">
            <img width="100%" alt="VAE Recon." src="images/dynamic_mnist_pixelhvae_2level_vampprior__K_500__wu_100__z1_40_z2_40/real_recon.png">
        </div>
        <div style="width: 30%; display:inline-block; vertical-align:middle;">
            <img width="100%" alt="VAE Z Embed" src="images/dynamic_mnist_pixelhvae_2level_vampprior__K_500__wu_100__z1_40_z2_40/z_embed.png">
        </div>
    </div>
    <p style="text-align: center;"><b>MNIST</b></p>    
</div>

<p>Code for this experiment is based on <a href="https://github.com/jmtomczak/vae_vampprior" target="_blank">Vamprior</a> paper</p>

<pre><code>@article{TW:2017,
  title={{VAE with a VampPrior}},
  author={Tomczak, Jakub M and Welling, Max},
  journal={arXiv},
  year={2017}
}
</code></pre>

<h2>Citation</h2>

<p>Please cite our <a href="https://arxiv.org/abs/1910.03175" target="_blank">paper</a> if you use this code in your research:</p>

<pre><code>@misc{livne2019mim,
    title={MIM: Mutual Information Machine},
    author={Micha Livne and Kevin Swersky and David J. Fleet},
    year={2019},
    eprint={1910.03175},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
</code></pre>

<h2>Acknowledgements</h2>

<p>Many thanks to Ethan Fetaya, Jacob Goldberger, Roger Grosse, Chris Maddison, 
and Daniel Roy for interesting discussions and for their helpful comments.
We are especially grateful to Sajad Nourozi for extensive discussions and for 
his help to empirically validate the formulation and experimental work.
This work was financially supported in part by the Canadian Institute for 
Advanced Research (Program on Learning in Machines and Brains), and NSERC Canada.</p>

<h2>Your Feedback Is Appreciated</h2>

<p>If you find this paper and/or repo to be useful, we would love to hear back!
Tell us your success stories, and we will include them in this README.</p>
    </div>
<div id="main-footer">
<p class="round-border">Generated with <a href="http://seraphlabs.ca" target="_blank">Seraph Labs</a> Open Source Framework</p>
</div>  </div>



<script type="text/javascript">$(document).ready(function(){var config={'.chosen-select':{search_contains:true,width:"90%"},}
for(var selector in config){$(selector).chosen(config[selector]);}});</script><script type="text/javascript">if(isTouchDevice()){$("#main").addClass("touch-screen");$("#main-header").addClass("touch-screen");}</script><script type="text/javascript">function resize_plots(){var fig_list=$('svg.mpld3-figure');$('svg.mpld3-figure').each(function(){var fig=this;var fig_parent=$(fig).closest('div.mpld3-resize-wrapper');var parent_width=$(fig_parent).width();var parent_height=$(fig_parent).height();if($(fig).data('orig-width')==undefined){$(fig).data('orig-width',$(fig).width());}
if($(fig).data('orig-height')==undefined){$(fig).data('orig-height',$(fig).height());}
var fig_width=$(fig).data('orig-width');var fig_height=$(fig).data('orig-height');var scale=Math.max(Math.min(parent_width/fig_width,parent_height/fig_height),0.1);$(fig).css('transform','scale('+scale+')');$(fig).css('-webkit-transform','scale('+scale+')');});}
$(window).resize(function(evt){resize_plots();});$(document).ready(function(){window.setTimeout(resize_plots,100);window.setTimeout(resize_plots,3000);});</script><script type="text/javascript">if(isTouchDevice()){$("#toc").addClass("touch-screen");}else{toggle_toc();}
function update_toc(){$('#toc-content').toc({'selectors':'h1,h2,h3','container':'div#main-content','listType':'<ol/>','smoothScrolling':false,'prefix':'toc','onHighlight':function(el){},'highlightOnScroll':true,'highlightOffset':0,'anchorName':function(i,heading,prefix){return prefix+i;},'headerText':function(i,heading,$heading){return $heading.text();},'itemClass':function(i,heading,$heading,prefix){return $heading[0].tagName.toLowerCase();}});if(isTouchDevice()){$("#toc-content").find("li").find("a").click(toggle_toc);}
$("#toc-content ol").searchable({selector:'li',childSelector:'a',searchField:'#toc-content-search',striped:false,show:function(elem){elem.removeClass('toc-hidden-search-element');},hide:function(elem){if(document.getElementById("toc-content-search").getAttribute('data-tem-len')>0){elem.addClass('toc-hidden-search-element');}else{elem.removeClass('toc-hidden-search-element');}},onSearchActive:function(elem,term){document.getElementById("toc-content-search").setAttribute('data-tem-len',term.length);},onSearchEmpty:function(elem){elem.find("li").removeClass('toc-hidden-search-element');},onSearchFocus:function(){$('#'+this.id).removeClass('search-toc-inactive');$('#'+this.id).addClass('search-toc-active');},onSearchBlur:function(){$('#'+this.id).removeClass('search-toc-active');$('#'+this.id).addClass('search-toc-inactive');},searchType:'fuzzy',clearOnLoad:true});}
$(document).ready(function(){window.setTimeout(update_toc,100);});function toggle_toc(){if($("#toc").hasClass("hidden")){$("#toc").toggleClass("hidden",0).toggleClass("toc-active toc-inactive",200);}else{$("#toc").toggleClass("toc-active toc-inactive",200).toggleClass("hidden",0);}
$("#main").toggleClass("main-with-toc main-without-toc",200);$("#toc-toggle-button").toggleClass("toc-button-active toc-button-inactive",200);};$("#toc-toggle-button").draggable().click(function(){if($(this).is('.ui-draggable-dragging')){return;}
toggle_toc();});window.addEventListener("keyup",function(event){switch(event.code){case"Escape":if(!event.ctrlKey){toggle_toc();}
break;}},false);</script>

</body>

</html>